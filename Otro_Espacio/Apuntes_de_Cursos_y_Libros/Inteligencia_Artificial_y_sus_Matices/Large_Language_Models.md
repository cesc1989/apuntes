# Large Language Models
Enlaces

- Definición en [Wikipedia](https://en.wikipedia.org/wiki/Large_language_model)
- [Large Language Models: Complete Guide in 2023](https://research.aimultiple.com/large-language-models/)


# Le pregunté a ChatGPT y dijo:
> Un "large language model" es un tipo de modelo de lenguaje que utiliza la inteligencia artificial para procesar y entender el lenguaje natural humano. Estos modelos están entrenados con grandes cantidades de datos lingüísticos y estadísticas de lenguaje para aprender a comprender y generar lenguaje humano.
> 
> El objetivo principal de los large language models es poder procesar y generar texto de manera autónoma, lo que se logra a través de técnicas de aprendizaje profundo y redes neuronales. Estos modelos son capaces de realizar tareas como la traducción automática, la generación de texto y el análisis de sentimientos.
> 
> (…)

Según [Wikipedia](https://es.wikipedia.org/wiki/Modelaci%C3%B3n_del_lenguaje), un **Modelo de Lenguaje** es aquel que mediante una distribución de probabilidad asigna una probabilidad a una secuencia de *m* palabras.

    P(W1...Wm)

Menciona las tareas en las que destacan:

- Traducción
- Generación de texto
- Análisis de datos


# Según el artículo LLM Complete Guide 2023

Los LLMs son modelos fundacionales que utilizan *deep learning* en tareas de [procesamiento de lenguajes naturales](https://es.wikipedia.org/wiki/Procesamiento_de_lenguajes_naturales) (NLP) y [generación de lenguajes naturales](https://es.wikipedia.org/wiki/Generaci%C3%B3n_de_lenguajes_naturales) (NLG).

Casos de uso:

- Resumen de textos
- Generación de textos
- Análisis de sentimientos
- Creación de contenidos
- Chatbots, asistentes virtuales e IA conversacional
- Reconocimiento de voz
- Anotación de imágenes
- Síntesis de texto a voz
- Corrección gramatical
- Sistemas de recomendación
- Detección de fraude
- Generación de código




## Modelo Fundacional

Un [Modelo fundacional](https://research.aimultiple.com/foundation-models/) es un tipo de modelo que ha sido entrenado de tal manera que puede usarse para ejecutar tareas específicas (*downstream tasks*) para las cuales no fue entrenado previamente.

![](https://paper-attachments.dropboxusercontent.com/s_7285799BA9F1090A7C4926777C821629688B69C17B1E7DB41EF63186095AA15E_1679890383951_Ekran+Resmi+2022-11-14+14.15.57.png)


En el gráfico vemos que un modelo fundacional se entren con datos como:

- textos
- imágenes
- grabaciones
- datos estructurados
- señales en 3d

y este podrá eventualmente ejecutar tareas como:

- responder preguntas
- análisis de sentimientos
- extracción de información
- subtitular imágenes
- reconocimiento de objetos
- seguimiento de instrucciones

Son popularmente conocidos como “self supervised” o “pre-trained models”.

